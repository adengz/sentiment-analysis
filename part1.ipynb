{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import pandas as pd\n",
    "\n",
    "from data import Vocabulary, get_dataloader\n",
    "from learner import SentimentLearner\n",
    "\n",
    "torch.manual_seed(41)\n",
    "loss_fn = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "EMBED_DIM = 200\n",
    "EMBED_DROPOUT = 0.5\n",
    "OPTIM_CLS = Adam\n",
    "LR = 5e-4\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.4 s, sys: 21.1 ms, total: 31.5 s\n",
      "Wall time: 31.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vocab = Vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = 'senti.{}.tsv'\n",
    "\n",
    "train_loader = get_dataloader(FILENAME.format('train'), vocab, batch_size=BATCH_SIZE)\n",
    "valid_loader = get_dataloader(FILENAME.format('dev'), vocab, batch_size=BATCH_SIZE)\n",
    "test_loader = get_dataloader(FILENAME.format('test'), vocab, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word averaging model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model and learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import WordAveragingModel\n",
    "\n",
    "wam = WordAveragingModel(len(vocab), embed_dim=EMBED_DIM, embed_dropout=EMBED_DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wam_learner = SentimentLearner(\n",
    "    model=wam,\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    loss_fn=loss_fn,\n",
    "    optim_cls=OPTIM_CLS,\n",
    "    lr=LR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 01\tWall time : 15.080s\n",
      "\tTrain Loss: 0.676 | Train Acc: 60.81%\n",
      "\tValid Loss: 0.634 | Valid Acc: 67.30%\n",
      "\tModel parameters saved to word_avg.pt\n",
      "Epoch : 02\tWall time : 14.908s\n",
      "\tTrain Loss: 0.592 | Train Acc: 77.70%\n",
      "\tValid Loss: 0.538 | Valid Acc: 75.35%\n",
      "\tModel parameters saved to word_avg.pt\n",
      "Epoch : 03\tWall time : 15.061s\n",
      "\tTrain Loss: 0.499 | Train Acc: 83.31%\n",
      "\tValid Loss: 0.485 | Valid Acc: 78.13%\n",
      "\tModel parameters saved to word_avg.pt\n",
      "Epoch : 04\tWall time : 14.928s\n",
      "\tTrain Loss: 0.426 | Train Acc: 86.26%\n",
      "\tValid Loss: 0.446 | Valid Acc: 80.79%\n",
      "\tModel parameters saved to word_avg.pt\n",
      "Epoch : 05\tWall time : 14.935s\n",
      "\tTrain Loss: 0.376 | Train Acc: 88.03%\n",
      "\tValid Loss: 0.427 | Valid Acc: 81.17%\n",
      "\tModel parameters saved to word_avg.pt\n",
      "Epoch : 06\tWall time : 14.717s\n",
      "\tTrain Loss: 0.338 | Train Acc: 89.19%\n",
      "\tValid Loss: 0.416 | Valid Acc: 81.33%\n",
      "\tModel parameters saved to word_avg.pt\n",
      "Epoch : 07\tWall time : 14.853s\n",
      "\tTrain Loss: 0.308 | Train Acc: 90.00%\n",
      "\tValid Loss: 0.411 | Valid Acc: 82.48%\n",
      "\tModel parameters saved to word_avg.pt\n",
      "Epoch : 08\tWall time : 14.811s\n",
      "\tTrain Loss: 0.285 | Train Acc: 90.60%\n",
      "\tValid Loss: 0.420 | Valid Acc: 81.42%\n",
      "\n",
      "Epoch : 09\tWall time : 14.926s\n",
      "\tTrain Loss: 0.266 | Train Acc: 91.26%\n",
      "\tValid Loss: 0.424 | Valid Acc: 82.13%\n",
      "\n",
      "Epoch : 10\tWall time : 14.959s\n",
      "\tTrain Loss: 0.250 | Train Acc: 91.67%\n",
      "\tValid Loss: 0.429 | Valid Acc: 81.81%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wam_filename = 'word_avg.pt'\n",
    "wam_learner.train(epochs=EPOCHS, filename=wam_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load best model to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wam_learner.load_model_params(wam_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Test Loss: 0.436 | Test Acc: 82.81%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = wam_learner.evaluate(test_loader)\n",
    "print(f'\\t Test Loss: {test_loss:.3f} | Test Acc: {test_acc * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Norm of word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "norms = pd.Series(torch.linalg.norm(wam_learner.model.word_embedding, dim=1), index=vocab.itos).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "beautifully    5.453632\n",
       "best           5.456155\n",
       "dull           5.578378\n",
       "touching       5.626331\n",
       "stupid         5.667773\n",
       "terrific       5.712662\n",
       "flat           5.726692\n",
       "remarkable     5.743237\n",
       "enjoyable      5.775650\n",
       "hilarious      5.823424\n",
       "powerful       5.900732\n",
       "solid          5.944018\n",
       "mess           6.177738\n",
       "bad            6.579552\n",
       "worst          7.167939\n",
       "dtype: float32"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norms.tail(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pad>        0.000000\n",
       "<unk>        0.020988\n",
       "alfonso      0.161497\n",
       "strategy     0.173768\n",
       "jeong        0.176887\n",
       "jae-eun      0.177959\n",
       "2/3          0.178415\n",
       "mikes        0.178877\n",
       "liman        0.180664\n",
       "boom         0.182392\n",
       "summary      0.186733\n",
       "opts         0.187388\n",
       "xiaoshuai    0.187785\n",
       "clubs        0.188298\n",
       "plate        0.190157\n",
       "dtype: float32"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norms.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention weighted word averaging model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model and learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import AttentionWeightedWordAveragingModel\n",
    "\n",
    "awwam = AttentionWeightedWordAveragingModel(len(vocab), embed_dim=EMBED_DIM, embed_dropout=EMBED_DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "awwam_learner = SentimentLearner(\n",
    "    model=awwam,\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    loss_fn=loss_fn,\n",
    "    optim_cls=OPTIM_CLS,\n",
    "    lr=LR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 01\tWall time : 15.407s\n",
      "\tTrain Loss: 0.671 | Train Acc: 59.74%\n",
      "\tValid Loss: 0.616 | Valid Acc: 71.09%\n",
      "\tModel parameters saved to atten_weighted_word_avg.pt\n",
      "Epoch : 02\tWall time : 15.548s\n",
      "\tTrain Loss: 0.580 | Train Acc: 76.86%\n",
      "\tValid Loss: 0.508 | Valid Acc: 76.98%\n",
      "\tModel parameters saved to atten_weighted_word_avg.pt\n",
      "Epoch : 03\tWall time : 15.463s\n",
      "\tTrain Loss: 0.483 | Train Acc: 83.03%\n",
      "\tValid Loss: 0.463 | Valid Acc: 79.00%\n",
      "\tModel parameters saved to atten_weighted_word_avg.pt\n",
      "Epoch : 04\tWall time : 15.307s\n",
      "\tTrain Loss: 0.413 | Train Acc: 86.27%\n",
      "\tValid Loss: 0.422 | Valid Acc: 81.99%\n",
      "\tModel parameters saved to atten_weighted_word_avg.pt\n",
      "Epoch : 05\tWall time : 15.321s\n",
      "\tTrain Loss: 0.363 | Train Acc: 88.15%\n",
      "\tValid Loss: 0.434 | Valid Acc: 80.45%\n",
      "\n",
      "Epoch : 06\tWall time : 15.446s\n",
      "\tTrain Loss: 0.326 | Train Acc: 89.53%\n",
      "\tValid Loss: 0.446 | Valid Acc: 81.61%\n",
      "\n",
      "Epoch : 07\tWall time : 15.303s\n",
      "\tTrain Loss: 0.296 | Train Acc: 90.36%\n",
      "\tValid Loss: 0.437 | Valid Acc: 81.72%\n",
      "\n",
      "Epoch : 08\tWall time : 15.264s\n",
      "\tTrain Loss: 0.275 | Train Acc: 91.06%\n",
      "\tValid Loss: 0.484 | Valid Acc: 81.43%\n",
      "\n",
      "Epoch : 09\tWall time : 15.307s\n",
      "\tTrain Loss: 0.256 | Train Acc: 91.53%\n",
      "\tValid Loss: 0.466 | Valid Acc: 83.35%\n",
      "\n",
      "Epoch : 10\tWall time : 15.422s\n",
      "\tTrain Loss: 0.241 | Train Acc: 91.99%\n",
      "\tValid Loss: 0.492 | Valid Acc: 82.49%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "awwam_filename = 'atten_weighted_word_avg.pt'\n",
    "awwam_learner.train(epochs=EPOCHS, filename=awwam_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load best model to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "awwam_learner.load_model_params(awwam_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Test Loss: 0.417 | Test Acc: 81.15%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = awwam_learner.evaluate(test_loader)\n",
    "print(f'\\t Test Loss: {test_loss:.3f} | Test Acc: {test_acc * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine similarities between vector u and word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarities = pd.Series(awwam_learner.model.cosine_similarity_to_u, index=vocab.itos).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "underbelly           0.725700\n",
       "morning              0.726054\n",
       "kaige                0.726750\n",
       "irreverent           0.729360\n",
       "exit                 0.731163\n",
       "kjell                0.731713\n",
       "foul                 0.731957\n",
       "sanctimonious        0.732995\n",
       "mcadams              0.741316\n",
       "élan                 0.746318\n",
       "lack-of-attention    0.753965\n",
       "mountain             0.755889\n",
       "inc.                 0.760517\n",
       "detract              0.775119\n",
       "buñuel               0.785297\n",
       "dtype: float32"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarities.tail(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the      -0.993271\n",
       "of       -0.992350\n",
       "is       -0.991175\n",
       "in       -0.989943\n",
       "'s       -0.988395\n",
       "its      -0.987115\n",
       "abroad   -0.987112\n",
       ".        -0.986810\n",
       "--       -0.985075\n",
       "your     -0.983277\n",
       "for      -0.982622\n",
       "hubert   -0.982240\n",
       "clubs    -0.980168\n",
       "junior   -0.975402\n",
       "...      -0.973906\n",
       "dtype: float32"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarities.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention variance among frequent words in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bisect\n",
    "from collections import defaultdict\n",
    "\n",
    "MAX_FREQ = 100\n",
    "upper_bound = len(vocab) - bisect.bisect_right(vocab.freqs[::-1], MAX_FREQ)\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_attention_stats(learner):\n",
    "    attentions = defaultdict(list)\n",
    "    for batch in train_loader:\n",
    "        sequences, _ = batch\n",
    "        sequences = sequences.to(learner.device)\n",
    "        \n",
    "        mask = torch.where(sequences < upper_bound, sequences, 0).bool()\n",
    "        attention = learner.model(sequences, True)\n",
    "        masked_sequences = torch.masked_select(sequences, mask).tolist()\n",
    "        masked_attention = torch.masked_select(attention, mask).tolist()\n",
    "        for i, att in zip(masked_sequences, masked_attention):\n",
    "            attentions[i].append(att)\n",
    "        \n",
    "    return attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.3 s, sys: 43.9 ms, total: 14.4 s\n",
      "Wall time: 14.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "stats = get_attention_stats(awwam_learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['word', 'mean', 'std'])\n",
    "for k, v in stats.items():\n",
    "    attentions = torch.Tensor(v)\n",
    "    df = df.append({'word': vocab.itos[k], 'mean': attentions.mean().item(), 'std': attentions.std().item()}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>neither</th>\n",
       "      <td>0.048591</td>\n",
       "      <td>0.008301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cold</th>\n",
       "      <td>0.049827</td>\n",
       "      <td>0.008294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>barely</th>\n",
       "      <td>0.050695</td>\n",
       "      <td>0.008227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cheap</th>\n",
       "      <td>0.048286</td>\n",
       "      <td>0.007940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trying</th>\n",
       "      <td>0.048671</td>\n",
       "      <td>0.007857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nor</th>\n",
       "      <td>0.049123</td>\n",
       "      <td>0.007704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>less</th>\n",
       "      <td>0.050165</td>\n",
       "      <td>0.007689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>left</th>\n",
       "      <td>0.048582</td>\n",
       "      <td>0.007632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>waste</th>\n",
       "      <td>0.051179</td>\n",
       "      <td>0.007504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lacks</th>\n",
       "      <td>0.051918</td>\n",
       "      <td>0.007453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tedious</th>\n",
       "      <td>0.049708</td>\n",
       "      <td>0.007373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plays</th>\n",
       "      <td>0.046173</td>\n",
       "      <td>0.007351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>none</th>\n",
       "      <td>0.052426</td>\n",
       "      <td>0.007321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lacking</th>\n",
       "      <td>0.049791</td>\n",
       "      <td>0.007256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video</th>\n",
       "      <td>0.046965</td>\n",
       "      <td>0.007207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nothing</th>\n",
       "      <td>0.048872</td>\n",
       "      <td>0.007202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lost</th>\n",
       "      <td>0.046647</td>\n",
       "      <td>0.007195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wrong</th>\n",
       "      <td>0.050141</td>\n",
       "      <td>0.007157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fails</th>\n",
       "      <td>0.050779</td>\n",
       "      <td>0.007125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seems</th>\n",
       "      <td>0.047664</td>\n",
       "      <td>0.007093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feels</th>\n",
       "      <td>0.048230</td>\n",
       "      <td>0.007084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seem</th>\n",
       "      <td>0.046197</td>\n",
       "      <td>0.007080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reason</th>\n",
       "      <td>0.049311</td>\n",
       "      <td>0.007069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rather</th>\n",
       "      <td>0.046696</td>\n",
       "      <td>0.007034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boring</th>\n",
       "      <td>0.049584</td>\n",
       "      <td>0.007016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>why</th>\n",
       "      <td>0.045696</td>\n",
       "      <td>0.007008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n't</th>\n",
       "      <td>0.049578</td>\n",
       "      <td>0.006982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>impossible</th>\n",
       "      <td>0.047068</td>\n",
       "      <td>0.006871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>were</th>\n",
       "      <td>0.049346</td>\n",
       "      <td>0.006867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>falls</th>\n",
       "      <td>0.049617</td>\n",
       "      <td>0.006860</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                mean       std\n",
       "word                          \n",
       "neither     0.048591  0.008301\n",
       "cold        0.049827  0.008294\n",
       "barely      0.050695  0.008227\n",
       "cheap       0.048286  0.007940\n",
       "trying      0.048671  0.007857\n",
       "nor         0.049123  0.007704\n",
       "less        0.050165  0.007689\n",
       "left        0.048582  0.007632\n",
       "waste       0.051179  0.007504\n",
       "lacks       0.051918  0.007453\n",
       "tedious     0.049708  0.007373\n",
       "plays       0.046173  0.007351\n",
       "none        0.052426  0.007321\n",
       "lacking     0.049791  0.007256\n",
       "video       0.046965  0.007207\n",
       "nothing     0.048872  0.007202\n",
       "lost        0.046647  0.007195\n",
       "wrong       0.050141  0.007157\n",
       "fails       0.050779  0.007125\n",
       "seems       0.047664  0.007093\n",
       "feels       0.048230  0.007084\n",
       "seem        0.046197  0.007080\n",
       "reason      0.049311  0.007069\n",
       "rather      0.046696  0.007034\n",
       "boring      0.049584  0.007016\n",
       "why         0.045696  0.007008\n",
       "n't         0.049578  0.006982\n",
       "impossible  0.047068  0.006871\n",
       "were        0.049346  0.006867\n",
       "falls       0.049617  0.006860"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sort_values('std', ascending=False)\n",
    "df = df.set_index('word', drop=True)\n",
    "df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
